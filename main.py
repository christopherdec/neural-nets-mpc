"""M√≥dulo principal

Cont√©m a fun√ß√£o main(), que controla o fluxo de execu√ß√£o do c√≥digo.
√â o arquivo que deve ser executado para cria√ß√£o e an√°lise dos modelos.
"""
from utils import data_utils as du
from utils import training_utils as tu
from utils import analysis_utils as au
from utils import plot_utils as pu
import tensorflow as tf


def main():
    """
    Controla o fluxo de execu√ß√£o e a cria√ß√£o e an√°lise dos modelos neurais

    Come√ßa definindo algumas configura√ß√µes do tensorflow, ent√£o importa os
    dados, os dicion√°rios, define as configura√ß√µes de execu√ß√£o e an√°lise,
    e chama as rotinas do pacote de utilidades.

    Os resultados da execu√ß√£o ficam nos dicion√°rios de treinamento, an√°lise e
    de modelos, que podem ser inspecionados com o Variable Explorer do Spyder.
    """
    tf.random.set_seed(0)
    # Reseta todo estado gerado pelo tensorflow
    tf.keras.backend.clear_session()
    # Suprime mensagens de warning do tensorflow
    tf.get_logger().setLevel('ERROR')
    tf.autograph.experimental.do_not_convert

    # Carregamento de dados, no exemplo √© usado o conjunto de dados levantados
    # com o modelo do simulink , que s√≥ tem uma sa√≠da 'y' e 4 vari√°veis de
    # entrada 'u'.
    raw_data, variable_names, Ny = du.load_data("simulink")

    # O dicion√°rio de treinamento cont√©m informa√ß√µes que v√£o sendo adicionadas
    # ao longo da cria√ß√£o dos modelos
    training_dictionary = du.load_pickle("analysis\\dictionary.pickle")

    # Para explorar os dicion√°rios, utilizar o Variable explorer do Spyder ou
    # o variableInspector do Jupyter Lab
    # Essa inspe√ß√£o √© imprescind√≠vel para analisar os resultados obtidos

    """ Configura√ß√µes de execu√ß√£o """

    # Se "create models": False, o loop de treinamentos √© pulado, indo direto
    # para a an√°lise e plots de modelos que j√° foram criados
    exec_cfg = {
        "create models": True,
        # first y e last y definem o intervalo de sa√≠das para as quais ser√£o
        # criados modelos.
        "first y": 1,
        "last y": Ny, # n„o precisa ser obrigatoriamente Ny
        "find inputs": True,  # pode-se usar False em casos de retomada
        "find K": True,  # se False, sÛ vai selecionar os inputs dos modelos
        "input selection params": {
            # usa todas as vari√°veis no input selection
            # se False, s√≥ considera as vari√°veis 'u' e a pr√≥pria sa√≠da
            "use all variables": False,
            # inicializa√ß√µes para cada op√ß√£o de inputs, diminui variabilidade
            "trains per option": 1,
            "max stages": 15,
            # define quantos est√°gios sem melhorias para interromper a busca
            "search patience": 2,
            # n√∫mero m√°ximo de √©pocas que um modelo √© treinado, normalmente
            # o callback earlystop ativa antes de chegar em max epochs
            "max epochs": 1000,
            "early stop patience": 3,
            # fixo nessa etapa, usar um valor razo√°vel
            "hidden layer nodes": 8,
            # horizonte de predi√ß√£o para esta etapa
            "horizon": 1,
            # ordem inicial das vari√°veis na vers√£o decremental, ou s√≥ de y na
            # vers√£o incremental (n√£o mais utilizada)
            "starting order": 3,
            # para datasets muito grandes. permite utilizar s√≥ uma parte para
            # o input selection
            "partition size": 1,
            # o input selection usa o train/validation split
            "validation size": 0.3,
            # interrompe a busca quando esse valor √© atingido
            "target loss": False,
            # interrompe a busca se nenhuma op√ß√£o melhorar o desempenho e esse
            # valor for atingido (menos restritivo)
            "acceptable loss": False,
            # delta m√≠nimo para considerar uma melhora como relevante
            "min delta loss": False,
            "structure": "DLP",   # DLP ou LSTM - geralmente DLP √© mais r√°pido
            "optimizer": "adam",  # SGD (stochastic gradient descent) ou adam
            "loss": "mse",        # Fun√ß√£o custo mean squared errors
            # Define o escalonador a ser utilizado: MinMax (s√≥ normaliza) ou
            # Standard (normaliza cada vari√°vel e divide pelo desvio padr√£o)
            "scaler": "Standard"
        },

        "K selection params": {
            "K min": 3,
            "K max": 10,
            "trains per K": 1,
            "search patience": 1,
            "max epochs": 1000,
            "early stop patience": 3,
            "horizon": 1,             # recomendado: 1, 20, 50, ...
            "partition size": 1,
            # K selection usa train/validation/test splits
            "validation size": 0.3,   # recomendado: 0.3
            "test size": False,       # recomendado: 0.15, 0 se horizon = 1
            "target loss": False,
            "min delta loss": False
        }
    }

    """ Configura√ß√µes de an√°lise """

    analysis_cfg = {
        "create analysis dict": True,
        # dicion√°rio para exporta√ß√£o dos modelos ao software externo
        "create model dict": True,
        "single plots": True,
        "multiplots": True,
        "multiplot size": [2, 2],
        "save plots": True,
        # permite realizar os plots com um horizonte diferente do que foi
        # utilizado no K selection, padr√£o = "default"
        "plots horizon": 50
    }

    """ In√≠cio da execu√ß√£o """

    if exec_cfg["create models"]:
        training_dictionary = tu.create_models(exec_cfg,
                                               training_dictionary,
                                               raw_data)

    # O dicion√°rio de an√°lise cont√©m informa√ß√µes sintetizadas a partir do
    # dicion√°rio de treinamento, ap√≥s sair do loop de treino
    if analysis_cfg["create analysis dict"]:
        analysis_dict = au.run_analysis(training_dictionary)
    else:
        analysis_dict = du.load_pickle("analysis\\analysis_dict.pickle")

    # O dicionÔøΩrio de modelos contÔøΩm informaÔøΩÔøΩes do modelos criados em um
    # formato combinado para exportaÔøΩÔøΩo ao programa externo que o converte
    # em XML, para poder entÔøΩo ser importado no MPA
    if analysis_cfg["create model dict"]:
        model_dict = au.create_model_dict(training_dictionary)
    else:
        model_dict = du.load_pickle("analysis\\model_dict.pickle")

    if analysis_cfg["single plots"]:
        loss_info = pu.single_plots(
            training_dictionary,
            raw_data,
            save=analysis_cfg["save plots"],
            horizon=analysis_cfg["plots horizon"])
        analysis_dict["plot evaluation"] = loss_info
        du.save_pickle(analysis_dict, "analysis\\analysis_dict.pickle")

    if analysis_cfg["multiplots"]:
        pu.multiplots(
            training_dictionary,
            raw_data,
            size=analysis_cfg["multiplot size"],
            save=analysis_cfg["save plots"],
            horizon=analysis_cfg["plots horizon"])

    return raw_data, variable_names, training_dictionary, analysis_dict, \
        model_dict


# "protective wrapper" que permite a importa√ß√£o deste arquivo pelo Sphinx
# para realizar a documenta√ß√£o sem executar o c√≥digo "solto"
if __name__ == "__main__":
    raw_data, variable_names, training_dictionary, analysis_dict, \
        model_dict = main()
